<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: 并行编程 | Yebangyu's Blog]]></title>
  <link href="http://www.yebangyu.org/blog/categories/bing-xing-bian-cheng/atom.xml" rel="self"/>
  <link href="http://www.yebangyu.org/"/>
  <updated>2018-04-06T21:26:00+08:00</updated>
  <id>http://www.yebangyu.org/</id>
  <author>
    <name><![CDATA[Yebangyu]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Singleton与多线程]]></title>
    <link href="http://www.yebangyu.org/blog/2016/12/25/singleton/"/>
    <updated>2016-12-25T18:42:32+08:00</updated>
    <id>http://www.yebangyu.org/blog/2016/12/25/singleton</id>
    <content type="html"><![CDATA[<p>如果您和我一样，都只有C++背景，之前对设计模式也一窍不通，那么也没有关系，因为本文不需要您对设计模式有多么了解。</p>

<h2 id="singleton">Singleton模式</h2>

<p>所谓的单例模式，single instance模式，简单实现如下：</p>

<p>```c++
class Singleton 
{
  public:
    static Singleton* instance();
  private:
    static Singleton* pInstance;
};</p>

<p>Singleton* Singleton::pInstance = NULL;</p>

<p>Singleton* Singleton::instance() 
{
  if (pInstance == NULL) {
    pInstance = new Singleton();
  }
  return pInstance;
}</p>

<p>```</p>

<!--more-->

<h2 id="singleton-1">多线程下的Singleton模式实现</h2>

<p>以上实现在单线程下运行良好（不考虑构造函数抛异常，注意，本文我们都不考虑C++ Exception）。如果是多线程呢？很自然的，我们想到可以加一把锁，或许就万事大吉了</p>

<p>```c++
Singleton* Singleton::instance() 
{
  LockGuard guard(yourlock);
  if (pInstance == NULL) {
    pInstance = new Singleton();
  }
  return pInstance;
  //guard析构，自动调用unlock
}</p>

<p>```</p>

<h2 id="double-check-locking">double check locking</h2>

<p>前面的实现中，存在一个问题：因为只初始化一次，因此大部分情况下其实pInstance都不会为空，只需要简单返回pInstance。既然如此，何必大部分情况下都需要加锁呢？毕竟锁会带来不小的代价的。因此就引入了double check locking，降低锁的使用和冲突，fast path的时候尽最大可能没有任何锁的使用和开销：</p>

<p>```c++
Singleton* Singleton::instance() 
{
  if (pInstance == NULL) {
    LockGuard guard(yourlock);
    if (pInstance == NULL) {
      pInstance = new Singleton();
    }
    //guard析构，自动调用unlock
  }
  return pInstance;
}</p>

<p>```
如你所见，当pInstance不为空时，不需要任何加锁、释放锁的动作，不过这回得两次检查pInstance是否为空。这是因为：假设线程1执行第三行，发现pInstance为空，还未执行第四行的时候，被调度了出去；线程2执行第三行，发现pInstance为空，获得锁，创建了实例，释放锁；当线程1重新恢复调度的时候，实例已经被创建，这时候它必须再次检查pInstance是否为空，否则就两次创建了，既不符合单例模式的语义，可能还会有其他更严重的问题（资源泄漏可能还算小事）。</p>

<h2 id="memory-reordering">memory reordering</h2>

<p>完美了？看起来很完美，但是问题很大。</p>

<p>注意其中的第6行：</p>

<p><code>pInstance = new Singleton();</code></p>

<p>它不是原子操作。这条语句，从逻辑和功能上包括三个部分：</p>

<p>1，分配内存空间，大小是sizeof(Singleton)</p>

<p>2，在该内存空间上调用Singleton的构造函数，完成初始化。</p>

<p>3，将pInstance指向该内存空间</p>

<p>信不信由你，编译器可能会对这三个步骤进行乱序，乱序后可能出现这样的情况：</p>

<p>```c++
Singleton* Singleton::instance() 
{
  if (pInstance == NULL) {
    LockGuard guard(yourlock);
    if (pInstance == NULL) {
      pInstance = operator new (sizeof(Singleton));
      new (pInstance) Singleton();
    }
    //guard析构，自动调用unlock
  }
  return pInstance;
}</p>

<p>```
也就是说，乱序后，编译器生成的代码的逻辑是：</p>

<p>1，分配内存单元。</p>

<p>2，将pInstance指向该内存空间。</p>

<p>3，在该内存空间调用构造函数，完成初始化。</p>

<p>乱序后，可能出现如下的data race：线程1执行完第六行后，pInstance不为空；线程2执行第三行判断，发现非空，返回，然而pInstance这时候还没有初始化，这几乎肯定会带来严重的问题。</p>

<h2 id="section">临时变量也于事无补</h2>

<p>你可能想使用一个temp指针来避开这个问题：</p>

<p>```c++
Singleton* Singleton::instance() 
{
  if (pInstance == NULL) {
    LockGuard guard(yourlock);
    if (pInstance == NULL) {
      Singleton *temp = new Singleton();
      pInstance = temp;
    }
    //guard析构，自动调用unlock
  }
  return pInstance;
}</p>

<p>```</p>

<p>这下总没有问题了吧？有问题。聪明的非常aggressive的编译器会认为temp在这里是毫无必要的，因此会将它去除，根本不引入任何的temp，乱序还是会发生。</p>

<p>哦，你突然想到，volatile可能可以帮你的忙。因此，你可能将程序变为：</p>

<p>```c++
class Singleton 
{
  public:
    static volatile Singleton* volatile instance();
  private:
    static volatile Singleton* volatile pInstance;
};</p>

<p>volatile Singleton* Singleton::volatile pInstance = NULL;
volatile Singleton* volatile Singleton::instance() 
{
  if (pInstance == NULL) {
    LockGuard guard(yourlock);
    if (pInstance == NULL) {
      volatile Singleton *volatile temp = new volatile Singleton();
      pInstance = temp;
    }
    //guard析构，自动调用unlock
  }
  return pInstance;
}</p>

<p>```</p>

<p>好多volatile，好吓人。简单解释一下：</p>

<p><code>volatile int * volatile p;</code></p>

<p>第一个volatile表明，p指向的内容随时可能会变</p>

<p>第二个volatile表明，p本身可能随时会变（p可能改变所指区域）</p>

<p>好了，temp也是volatile，我们试图这样，让编译器不要乱序。</p>

<p>然而，这是compiler dependent的，它可能会work，也可能不会work。如果它碰巧在我们的环境下是有效的，只能说我们是幸运的，然而它显然不可移植了。</p>

<h2 id="section-1">明确大胆的说出来</h2>

<p>不就是想防止编译器乱序吗？用了那么多volatile看得真心很烦。其实，我们需要的仅仅是一个memory barrier，防止编译器乱序。因此，我们应该显式的告诉编译器，比如这样(以gcc为例)：</p>

<p>```c++
Singleton* Singleton::instance() 
{
  if (pInstance == NULL) {
    LockGuard guard(yourlock);
    if (pInstance == NULL) {
      Singleton *temp = new Singleton();
      <strong>asm</strong> <strong>volatile</strong>(“” : : : “memory”);
      pInstance = temp;
    }
    //guard析构，自动调用unlock
  }
  return pInstance;
}</p>

<p>```
是的，这里需要一个memory barrier，告诉编译器不要乱序。</p>

<p>即使这么做了，可能还是有问题。怎么说？C++标准并没有对多个线程读取同一个变量会发生什么进行任何规定。因此，上面的第3、第5、第8行存在data race，因此这是一个未定义的、不可移植的行为。</p>

<p>BTW，在X86下，问题不大，只要该指针是cacheline aligned即可。</p>

<p>让我们来分析下，在X86下，第3、5、8行的读写同步问题。假设该指针已经cacheline aligned，因此第3、5、8行的读写是原子的。</p>

<p>如果cpu 0上的线程1执行了第8行设置操作，在cpu 1上运行的线程2执行第3行。注意，这时候pInstance的新值对线程2不一定可见。然而这里不会有问题，因为：</p>

<p>1，假如可见，那么第3行if判断不成立，成功返回instance。</p>

<p>2，假如不可见，那么第3行if判断成立，此时pInstance实际上已经不为空，实例已经被创建。线程2尝试加锁操作，成功后，在第5行必然看到pInstance不为空，也就是看到pInstance的新值。</p>

<p>这是由锁的语义保证的：加锁解锁构成了一种synchronize with关系，加锁线程不管处在哪个处理器上，都可以看到上一个不管在哪个处理器上的释放锁的线程对变量的更新后的新值。</p>

<p>值得注意的是，为了防止第3行、第5行的读取pInstance的动作被编译器优化，可以使用<code>ACCESS_ONCE</code>宏。细节可以参考我的<a href="http://www.yebangyu.org/blog/2015/10/31/linux-parallen-programmming-infrastructure/">这篇</a>博客。</p>

<h2 id="c11">C++11来解救</h2>

<p>写出可移植的、线程安全的Singleton，这么难么？在C++11之前，很难。非常难。</p>

<p>现在，有了C++11，有了C++11的memory model，就不算太难了。</p>

<p>熟悉memory model的同学知道，其实我们这里需要的是一个acquire release语义。设置pInstance的时候使用release语义，读取pInstance的时候使用acquire语义。简单实现如下：</p>

<p>```c++
class Singleton 
{
  public:
    static Singleton* instance();
  private:
    static atomic&lt;Singleton*&gt; pInstance;
};</p>

<p>atomic&lt;Singleton*&gt; Singleton::pInstance;</p>

<p>Singleton* Singleton::instance() 
{
  Singleton* temp = pInstance.load(std::memory_order_acquire);
  if (temp == NULL) {
    LockGuard guard(yourlock);
    temp = pInstance.load(std::memory_order_acquire);
    if (temp == NULL) {
      temp = new Singleton();
      pInstance.store(temp, std::memory_order_release);
    }
    //guard析构，自动调用unlock
  }
  return temp;
}</p>

<p>```</p>

<p>思考：第16行能否使用<code>memory_order_relaxed</code>内存序？</p>

<h2 id="section-2">拓展</h2>

<p>1，查阅资料，了解POSIX中，<code>pthread_once</code>的原理和用法</p>

<p>2，使用volatile的版本中，我们对指针和指针所指之物都加了volatile，也就是：</p>

<p><code>volatile Singleton *volatile p</code></p>

<p>能否改为</p>

<p><code>volatile Singleton *p</code></p>

<p>或者</p>

<p><code>Singleton * volatile p</code></p>

<p>3，查阅资料，解释为什么编译器可能对 <code>pInstance = new Singleton()</code> 进行乱序</p>

<p>4，我们一直没有涉及的一个问题是：pInstance的初始化。它的初始化，是否也可能存在data race？如何避免？</p>

<p>5，不少朋友可能想利用C++11标准中的某个用法：</p>

<p><code>c++
Singleton&amp; instance()
{
  static Singleton xx; //C++11规定，这个是线程安全的。
  return xx;
}
</code></p>

<p>然而这么做有一些缺点，这些缺点使得这种做法往往不是实现Singleton单例模式的可行方案。读者诸君不妨思考下，这种做法，都有哪些缺点。</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Lock Free中的Hazard Pointer(下)]]></title>
    <link href="http://www.yebangyu.org/blog/2016/12/04/introductiontohazardpointer/"/>
    <updated>2016-12-04T17:29:10+08:00</updated>
    <id>http://www.yebangyu.org/blog/2016/12/04/introductiontohazardpointer</id>
    <content type="html"><![CDATA[<p>在前面几篇<a href="http://www.yebangyu.org/blog/2015/12/10/introduction-to-hazard-pointer/">文章</a>里，我们介绍了Hazard Pointer，一种用于lock free data structure设计中内存管理的利器，这个利器不仅思想简单，还可以用来防止ABA问题，读者诸君务必掌握。</p>

<p>本文作为第三部分，给出工业级代码中，实现Hazard Pointer的一些策略和需要注意的点。</p>

<h2 id="hazard-pointer">Hazard Pointer管理</h2>

<p>我们知道Hazard Pointer封装了原始指针，那么Hazard Pointer的内存和生命周期本身如何管理呢？以下是常见的策略：</p>

<!--more-->

<p>1，Hazard Pointer本身的内存只分配，不释放。在stack、queue等数据结构里，需要的Hazard Pointer数量一般为1或者2，所以不释放问题不大。对于skip list这种数据结构又有遍历需求的，那么Hazard Pointer可能就不是非常适用了，可以考虑使用<a href="http://www.yebangyu.org/blog/2016/09/09/epochbasedreclamation/">Epoch Based Reclamation</a>技术。据我所知，这也是memsql使用的内存回收策略。</p>

<p>2，每个线程拥有、管理自己的retire list和hazard pointer list ，而不是所有线程共享一个retire list，这样可以避免维护retire list和hazard pointer list的开销，否则我们可能又得想尽脑汁去设计另外一套lock free的策略来管理这些list，先有鸡先有蛋，无穷无尽。所谓retire list就是指逻辑删除后待物理回收的指针列表。</p>

<p>3，每个线程负责回收自己的retire list中记录维护的内存。这样，retire list是一个线程局部的数据结构，自己写，自己读，吃自己的狗粮。</p>

<p>4，只有当retire list的大小（数量）达到一定的阈值时，才进行GC。这样，可以把GC的开销进行分摊，同时，应该尽可能使用Jemalloc或者TCmalloc这些高效的、带线程局部缓存的内存分配器。</p>

<h2 id="acquirerelease">acquire和release动作</h2>

<p>所谓acquire，就是线程需要对一个资源进行访问，需要对它进行保护；所谓release，就是线程停止对资源的访问，结束对它的保护。</p>

<p>这两个动作基本上都是成对出现的，因此，可以封装成一个Guard。</p>

<p>```c++
   struct HazardPointerRecord
    {
      HazardPointerRecord(HplfdsHazardPointer<memoryallocator> &amp;manager,
                          void *p, int thread_id): manager_(manager)
      {
        p_ = p;
        thread_id_ = thread_id;
        manager.acquire(p_, thread_id_);
      }
      ~HazardPointerRecord()
      {
        manager_.release(p_, thread_id_);
      }
      HplfdsHazardPointer<memoryallocator> &manager_;
      void *p_;
      int thread_id_;
    };</memoryallocator></memoryallocator></p>

<pre><code>#define ACQUIRE_POINTER(p)\   HazardPointerRecord record##p(memory_manager_, p, thread_id);
</code></pre>

<p>```
更多细节，可以参考业余时间编写的<a href="https://github.com/yebangyu/HPLFDS/blob/master/src/queue/hplfds_ms_queue.h">lock free ms queue</a>中的用法。</p>

<h2 id="section">实现</h2>

<p>我在我的<a href="https://github.com/yebangyu/HPLFDS">HPLFDS</a>开源项目中，给出了一个<a href="https://github.com/yebangyu/HPLFDS/blob/master/src/hazardpointer/hplfds_hazard_pointer.h">实现</a>。虽然reclaim可以做的更加高效，然而在大部分场景下已经足够用了。</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[一个轻量的lock free程序调试工具]]></title>
    <link href="http://www.yebangyu.org/blog/2016/10/13/debuggerforlockfreeprogamming/"/>
    <updated>2016-10-13T22:39:29+08:00</updated>
    <id>http://www.yebangyu.org/blog/2016/10/13/debuggerforlockfreeprogamming</id>
    <content type="html"><![CDATA[<p>lock free 程序时序混乱，逻辑复杂，反直觉的地方很多，心智负担很重，因此调试起来也非常困难。</p>

<p>下面用C++编写了一个用于调试lock free代码的程序，非常轻量，代码如下，只适用于linux环境。</p>

<p>```c++
//xx.h
#include <pthread.h>
class LockFreeLogger
{
public:
  struct Event
  {
    char *message;
    pthread_t thread_id;
    int info;
  };
  LockFreeLogger()
  {
    pos_ = 0;
    buffer_ = new Event[SIZE];
  }
  void log(char *message, int info)
  {
    int pos = __sync_fetch_and_add(&amp;pos_, 1);
    buffer_[pos].message = message;
    buffer_[pos].info = info;
    buffer_[pos].thread_id = pthread_self();
  }
  void report()
  {
    //single thread code
    for (int i = 0; i &lt; pos_; i++) {
      //cout or fwrite buffer_[i];
    }
  }
private:
  int pos_;
  Event *buffer_;
  static const int SIZE = 1234567;
};</pthread.h></p>

<p>extern LockFreeLogger logger;</p>

<h1 id="define-logmessage-info-loggerlogmessage-info">define LOG(message, info) logger.log(message, info)</h1>

<p>```</p>

<!--more-->

<p>当然，为了能使用它，应该在某个cpp里面定义一个实例出来</p>

<p><code>c++
//xx.cpp
#include "xx.h"
LockFreeLogger logger;
</code></p>

<p>然后就可以使用<code>LOG(message, info)</code>来记录信息了。比如说<code>LOG("current value of counter", id)</code>等。读者诸君可以根据需要，自由拓展Event结构。</p>

<h2 id="section">注意事项</h2>

<p>1，<code>LockFreeLogger</code>构造函数里的内存分配操作可能会失败，工业级代码需要考虑异常或者错误处理。</p>

<p>2，数组下标可能会越界，达到或者超过SIZE。不过调试的时候可以考虑把SIZE设大一点，一般情况下也就够用了。</p>

<p>3，注意到这个调试程序有时候并不能work。为什么？举个简单的场景：假如你的lock free程序中存在一个bug，也就是两条语句之间漏加了memory barrier，导致乱序。</p>

<p>你没发觉，并在两条语句中间加了日志，想看看变量的值，结果因为加了代码，乱序不出现，bug不出现了。</p>

<p>4，编写这个程序的思想，是我自己想出来的。后来我在《Multi Core Programming》这本书的第八章看到了同样的idea，又后来看到了Jeff Preshing写了一篇<a href="http://preshing.com/20120522/lightweight-in-memory-logging/">博客</a>，介绍了一样的想法和做法，不过他的代码是针对Windows环境的。</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Lock Free中的Epoch Based Reclamation]]></title>
    <link href="http://www.yebangyu.org/blog/2016/09/09/epochbasedreclamation/"/>
    <updated>2016-09-09T22:24:02+08:00</updated>
    <id>http://www.yebangyu.org/blog/2016/09/09/epochbasedreclamation</id>
    <content type="html"><![CDATA[<h2 id="section">楔子</h2>

<p>一般认为，用C/C++编写Lock Free代码非常困难，主要原因无非是两个：</p>

<blockquote>
  <ul>
    <li>内存模型</li>
  </ul>
</blockquote>

<blockquote>
  <ul>
    <li>内存回收</li>
  </ul>
</blockquote>

<p>C++11引入了标准的内存模型，在此之前，C++程序员依赖于具体的体系结构特点和编译器提供的feature来保证正确的内存访问语义。C++11出来后，程序员编写健壮的、可移植的lock free代码成为可能。</p>

<p>但是内存回收问题依旧存在。我们知道，和Java这种提供自动gc的语言相比，C++程序员刀耕火种，得自己管理内存。当一个线程正在访问某块内存，而另外一个线程将它释放将是一个灾难行为。解决内存回收问题在lock free里显得更加困难。</p>

<p>目前用于lock free代码的内存回收的经典方法有：Lock Free Reference Counting、Hazard Pointer、Epoch Based Reclamation、Quiescent State Based Reclamation等。<a href="http://www.yebangyu.org/blog/2015/12/10/introduction-to-hazard-pointer/">上回</a>我们简单介绍了Hazard Pointer，本文我们介绍Epoch Based Reclamation方法。据我所知，这是第一篇介绍这个方法的中文资料。</p>

<!--more-->

<h2 id="section-1">概念</h2>

<p>1，逻辑删除和物理删除：逻辑删除仅仅是在逻辑上删除该节点，该节点在被逻辑删除之时可能会有其他线程正在访问它，而逻辑删除之后不会再被线程访问到。逻辑删除不回收内存空间。物理删除则是将对应的内存空间回收。一般逻辑删除对应delete，物理删除对应free或者reclaim。</p>

<p>2，Grace Period：记时间段T＝[t1,t2]，如果t1之前逻辑删除的节点，都可以在t2之后安全的回收，那么称T是一个Grace Period。t2之后保证不会有任何线程会访问在t1之前逻辑删除的节点。</p>

<p>3，临界区：在本文，临界区指的是线程访问共享内存资源的代码段。和传统上所说的临界区意义不同。</p>

<h2 id="section-2">实现</h2>

<p>我们依然先给出实现，再讲原理。注意到这里为了能focus我们的主题，我们刻意简化为四个线程，其中三个读线程，它们对数据都是只读的；而只有一个写线程可以对资源进行改写和删除，因此不需要加锁。</p>

<p>也请注意，以下伪代码只起示范作用，离真正生产环境实现还差很远。尽管如此，我们还是提供了一些实现细节和关键点(参考最后的思考一节)。</p>

<p><code>c++
#define N_THREADS 4 //一共4个线程
bool active[N_THREADS] = {false};
int epoches[N_THREADS] = {0};
int global_epoch = 0;
vector&lt;int*&gt; retire_list[3];
void read(int thread_id)
{
  active[thread_id] = true;
  epoches[thread_id] = global_epoch;
  //进入临界区了。可以安全的读取
  //...... 
  //读取完毕，离开临界区
  active[thread_id] = false;
}
void logical_deletion(int thread_id)
{
  active[thread_id] = true;
  epoches[thread_id] = global_epoch;
  //进入临界区了，这里，我们可以安全的读取
  //好了，假如说我们现在要删除它了。先逻辑删除。
  //而被逻辑删除的tmp指向的节点还不能马上被回收，因此把它加入到对应的retire list
  retire_list[global_epoch].push_back(tmp);
  //离开临界区
  active[thread_id] = false;
  //看看能不能物理删除
  try_gc();
}
bool try_gc()
{
  int &amp;e = global_epoch;
  for (int i = 0; i &lt; N_THREADS; i++) {
    if (active[i] &amp;&amp; epoches[i] != e) {
        //还有部分线程没有更新到最新的全局的epoch值
        //这时候可以回收(e + 1) % 3对应的retire list。
        free((e + 1) % 3);//不是free(e)，也不是free(e-1)。参看下面
        return false;
    }
  }
  //更新global epoch
  e = (e + 1) % 3;
  //更新之后，那些active线程中，部分线程的epoch值可能还是e - 1（模3）
  //那些inactive的线程，之后将读到最新的值，也就是e。
  //不管如何，(e + 1) % 3对应的retire list的那些内存，不会有人再访问到了，可以回收它们了
  //因此epoch的取值需要有三种，仅仅两种是不够的。
  free((e + 1) % 3);//不是free(e)，也不是free(e-1)。参看下面
}
bool free(int epoch)
{
  for each pointer in retire_list[epoch]
    if (pointer is not NULL)
      delete pointer;
}
</code></p>

<h2 id="section-3">原理</h2>

<p>算法维护了一个全局的epoch(取值为0、1、2)和三个全局的retire_list（每个全局的epoch对应一个retire list, retire list 存放逻辑删除后待回收的节点指针）。除此之外我们为每个线程维护一个局部的active flag和epoch(取值自然也为0、1、2)。</p>

<h3 id="section-4">读线程</h3>

<p>首先设置自己的active flag 为true，表明自己需要访问共享内存。然后将自己的局部的epoch设置为全局的epoch值。</p>

<p>这之后线程就进入临界区，可以放心的读取资源了，不用担心内存会被回收。</p>

<p>离开临界区时，将自己的active flag设置为false即可。</p>

<h3 id="section-5">写线程</h3>

<p>这里说的写线程是指会对资源进行删除的线程。首先进行逻辑删除，然后尝试对它进行物理删除，也就是回收。</p>

<p>令全局的epoch值为e。首先检查，如果存在活动线程的局部epoch值不等于全局epoch值，那么可以回收retire_list[(e+1) % 3]。</p>

<p>如果不存在，那么我们把全局epoch的值更新为(e+1)%3，然后再回收对应的retire list。</p>

<p>有点绕？这里关键是理清线程局部epoch和全局epoch的关系：在这个算法里，任何时刻，全局epoch的值如果为e，那么线程的局部epoch值要么也为e，要么为e-1，不可能为e+1。</p>

<p>所以[epoch, epoch + 2]（模3）构成了一个Grace Period。</p>

<p>当全局epoch的值为1的时候，线程的局部epoch要么是1，要么是0，不可能是2。因此[2,1]构成了一个Grace Period，也就是说2对应的retire list这时候可以放心的回收了。</p>

<p>当全局epoch的值为0的时候，线程的局部epoch要么是0，要么是2，不可能是1。因此[1,0]构成了一个Grace Period，也就是说1对应的retire list这时候可以放心的回收了。</p>

<p>当全局epoch的值为2的时候，线程的局部epoch要么是2，要么是1，不可能是0。因此[0,2]构成了一个Grace Period，也就是说0对应的retire list这时候可以放心的回收了。</p>

<p>也因此，当全局epoch值为2时，如果存在部分活跃线程的局部epoch等于1，那么retire_list[1]将不能立马回收。如果这些活跃线程不退出临界区（在里面不断的读、疯狂的读，或者死了），那么retire_list[1]纪录的那些内存将一直无法回收。</p>

<p>这也就是在Epoch Based Reclamation中，回收操作可能被读延迟的原因所在。</p>

<h2 id="section-6">思考</h2>

<p>1，代码中8、9两行是写一个变量(active[i])，读另外一个变量(global_epoch)，在x86下可能会乱序，这里是否需要一个cpu级别的memory barrier？如果被乱序，有没有影响？</p>

<p>2，<code>retire_list[global_epoch].push_back(tmp);</code> 能否改为 <code>retire_list[epoches[thread_id]].push_back(tmp);</code> ？也就是说这里能否改为使用线程局部epoch值？</p>

<p>3，和Hazard Pointer相比，Epoch Based Reclamation 有什么优点和缺点？</p>

<p>4，为什么epoch有三个取值0、1、2？能不能仅使用两个？</p>

<p>5，线程局部变量我们使用了<code>active</code>数组和<code>epoches</code>数组，这里会有性能问题，为什么？提示：<a href="http://www.yebangyu.org/blog/2015/12/30/falsesharing/">False Sharing</a></p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[MCS Lock]]></title>
    <link href="http://www.yebangyu.org/blog/2016/08/21/mcslock/"/>
    <updated>2016-08-21T20:25:11+08:00</updated>
    <id>http://www.yebangyu.org/blog/2016/08/21/mcslock</id>
    <content type="html"><![CDATA[<h2 id="section">回顾</h2>

<p><a href="http://www.yebangyu.org/blog/2016/05/26/ticketlock/">上回</a> 我们介绍了Ticket Lock算法，和传统的简单的CAS操作来实现spin lock相比，它提供了不少很好的性质：比如FIFO、没有饥饿等等。</p>

<p>但是根据<a href="https://pdos.csail.mit.edu/papers/linux:lock.pdf">论文</a>，ticket lock的scalability很差。我们不妨简单回顾一下（代码片段也摘自该论文）：</p>

<p><code>c++
struct spinlock_t 
{
  int current_ticket ;
  int next_ticket ;
}
void spin_lock (spinlock_t *lock)
{
  int t = atomic_fetch_and_inc(&amp;lock -&gt; next_ticket );
  while (t != lock-&gt;current_ticket )
  ; /* spin */
}
void spin_unlock (spinlock_t *lock)
{
  lock-&gt;current_ticket++;
}
</code></p>

<!--more-->

<p>正如论文中指出的那样，当某个core上持有锁的线程释放锁时，它将把其他core上的<code>current_ticket</code>对应的cacheline都invalid掉，之前所有等锁的线程都会通过bus来读取最新的cacheline，这将造成“拥堵”(也就是所谓的bus traffic)。由于在绝大多数体系结构下，这些读取请求都会被串行化，one by one的处理，因此<code>spin_lock</code>所需时间将正比于等待锁的线程数目。</p>

<p>本质上，这里的问题还是在于资源共享：所有等锁线程都spin在同一个全局变量上。如果每个线程仅仅spin在本地变量(该线程私有的变量)，那么将有效的提高scalability。本文要介绍的MCS Lock就是这样的思路。</p>

<h2 id="section-1">实现</h2>

<p>这次，我们先给出实现（环境为：GCC 4.8 + X86体系结构 + Ubuntu 14.04 32bit系统），然后再讲解算法。</p>

<p>```c++
#define COMPILER_BARRIER() <strong>asm</strong> <strong>volatile</strong>(“” : : : “memory”)
#define CPU_RELAX() <strong>asm</strong> <strong>volatile</strong>(“pause\n”: : :”memory”)
#define CAS(address, exp, target) __sync_bool_compare_and_swap(address, exp, target)
#define ATOMIC_EXCHANGE(address, val) __atomic_exchange_n(address, val, __ATOMIC_SEQ_CST)
#define CPU_BARRIER() __sync_synchronize()
//=========================================</p>

<p>struct mcs_lock_node 
{
  volatile int waiting;
  mcs_lock_node *volatile next;
};</p>

<p>typedef mcs_lock_node *volatile mcs_lock;</p>

<p>static mcs_lock_node* get_my_mcs_node()
{
  static __thread mcs_lock_node my_mcs_node;
  return &amp;my_mcs_node;
}</p>

<p>static void spin_lock(mcs_lock &amp;lock)
{
  mcs_lock_node *me = get_my_mcs_node();
  mcs_lock_node *tmp = me;
  mcs_lock_node *pre;
  me-&gt;next = NULL;
  pre = ATOMIC_EXCHANGE(&amp;lock, tmp);                            <br />
  if (pre == NULL) {
    return;	                                           <br />
  }
  me-&gt;waiting = 1;                                             <br />
  COMPILER_BARRIER();
  pre-&gt;next = me;                                              <br />
  while (me-&gt;waiting) {                                       <br />
    CPU_RELAX();
  }
  CPU_BARRIER();
}</p>

<p>static void spin_unlock(mcs_lock &amp;lock)
{
  CPU_BARRIER();
  mcs_lock_node *me = get_my_mcs_node();
  mcs_lock_node *tmp = me;
  if (me-&gt;next == NULL) {                                    <br />
    if (CAS(&amp;lock, tmp, NULL)) {<br />
      return;
    }
    while (me-&gt;next == NULL) {                        <br />
      CPU_RELAX();
    }
  }
  me-&gt;next-&gt;waiting = 0; 
}
```</p>

<h2 id="section-2">算法</h2>

<p>MCS Lock算法维护一个队列(队尾指针tail)。</p>

<p>lock：尝试加锁的线程首先获得自己的节点，接着通过原子操作<code>ATOMIC_EXCHANGE</code>，将自己插入队列中，并获得插入前队尾指针tail。如果tail指针为空，说明当前锁是空闲的，没有被任何线程占用，因此获得锁成功；如果tail指针不空，那么设置自己的前驱，然后自旋，等待它的前驱(线程)将自己的<code>waiting</code>域置为0。<code>waiting</code>域一旦为0，那么线程结束spin，该线程就变成锁的持有者了。</p>

<p>unlock：释放锁的线程需要检查是否存在后继，如果存在后继，那么将它的后继的<code>waiting</code>域设置为0即可，后继将结束spin，成功获得锁。一切搞定，如54行代码所示。</p>

<p>如果不存在后继，那么这里需要特别特别注意。不存在后继有两种可能：</p>

<p>它是最后一个线程。47-49行处理的就是这种情况。那么这时候只需要把队列置为空即可。</p>

<p>它不是最后一个线程。那么此时存在另外一个线程在进行加锁操作，尝试加锁的线程已经通过28-33行设置了新的队尾指针，但是还没设置它的前驱，也就是还没执行到34行。因此释放锁的线程需要等待，等待34行被其他线程执行，等待它被人设置为别人的前驱。这也就是50-52行所做的事。</p>

<p>简单说来，MCS Lock就是将线程们用队列管理起来；加锁的线程spin在自己的变量上；释放锁的线程只更新它后继的变量。</p>

<h2 id="section-3">练习</h2>

<p>1，MCS Lock的优点是显而易见的，因为每个等待锁的线程都spin在自己的局部的变量上。以至于新版的linux内核已经开始采用它，来代替之前的ticket lock实现。那么，它的缺点呢？</p>

<p>2，列出表格，从公平性、是否出现饥饿、时间空间复杂度等方面，全面对比ticket lock、mcs lock、peterson lock等。</p>

<p>3，编写程序测试这几个算法的性能，考察它们的scalability。</p>

<h2 id="section-4">附录</h2>

<p>MCS Lock测试程序Demo</p>

<p><code>c++
#include&lt;pthread.h&gt;
mcs_lock global_lock = NULL;
void* f(void *arg)
{
  for(int i = 0; i &lt; 1000000; i++) {
    spin_lock(global_lock);
    spin_unlock(global_lock);
  }
  return NULL;
}
int main()
{
  pthread_t pthread1;
  pthread_t pthread2;
  pthread_create(&amp;pthread1, NULL, f, NULL);
  pthread_create(&amp;pthread2, NULL, f, NULL);
  pthread_join(pthread1, NULL);
  pthread_join(pthread2, NULL);
  return 0;
}
</code></p>

]]></content>
  </entry>
  
</feed>
